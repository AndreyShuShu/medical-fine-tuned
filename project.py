# -*- coding: utf-8 -*-
"""my_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IR8iU-SYK0iEI1gO3m5iG417K4YApgGb
"""

# libraries import
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
from itertools import chain
from functools import partial
os.environ["SUNO_USE_SMALL_MODELS"] = "True"
os.environ["SUNO_OFFLOAD_CPU"] = "True"

torch.cuda.empty_cache()

# medical dataset with 5000 examples taken
dataset = load_dataset("FreedomIntelligence/medical-o1-verifiable-problem", split="train[:5000]")

# LLM model with 1.3 billion parameters
model_name = "EleutherAI/gpt-neo-1.3B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# dataset preprocessing
def formatting(sample):
  return {
    "text": f"Prompt:\n{sample['Open-ended Verifiable Question']}\n\nAnswer:\n{sample['Ground-True Answer']}"
  }

def format_dataset(sample):
  sample["text"] = f"{formatting(sample)}{tokenizer.eos_token}"
  return sample

dataset = dataset.map(format_dataset)
dataset = dataset.remove_columns([col for col in dataset.column_names if col != "text"])

print(dataset['text'][5])

# chunking a dataset to speed up the training
remainder = {"input_ids": [], "attention_mask": []}

def chunk(sample, chunk_length=512):
    global remainder
    concatenated = {k: list(chain(*sample[k])) for k in sample.keys()}
    concatenated = {k: remainder[k] + concatenated[k] for k in sample.keys()}
    total_length = len(concatenated["input_ids"])
    usable_length = (total_length // chunk_length) * chunk_length
    result = {
        k: [t[i: i + chunk_length] for i in range(0, usable_length, chunk_length)]
        for k, t in concatenated.items()
    }
    remainder = {k: concatenated[k][usable_length:] for k in concatenated.keys()}
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = dataset.map(
    lambda sample: tokenizer(sample["text"], return_attention_mask=True),
    batched=True,
    remove_columns=["text"],
)

lm_dataset = tokenized_dataset.map(
    partial(chunk, chunk_length=512),
    batched=True,
)

# splitting a dataset into train, test and validation subsets (75% of train data, 12.5% - test and validation)
dataset_split = lm_dataset.train_test_split(test_size=0.25, seed=42)
train_dataset, eval_test_dataset = dataset_split['train'], dataset_split['test']

eval_test_split = eval_test_dataset.train_test_split(test_size=0.5, shuffle=False)
eval_dataset, test_dataset = eval_test_split['train'], eval_test_split['test']

del dataset_split, eval_test_split, eval_test_dataset

train_dataset.shape, eval_dataset.shape, test_dataset.shape

# setting the training arguments for the train, epoch strategy for gaining the checkpoints and train/validation losses every epoch,
# setting effective batch size to 16 to improve the generalization of the model, also fp16 for speeding up the training
training_args = TrainingArguments(
    output_dir="/mnt/d/medical-finetuned",
    eval_strategy="epoch",
    report_to="none",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    eval_accumulation_steps=16,
    save_strategy="epoch",
    metric_for_best_model="loss",
    save_total_limit=1,
    load_best_model_at_end=True,
    num_train_epochs=3,
    learning_rate=2e-5,
    fp16=True
)

# collator for batch preparing for the training
collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# object trainer with the arguments given in training_args and train/validation datasets
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collator,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# start the training
trainer.train()

# saving the model
trainer.save_model("/mnt/e/medical-finetuned-done")
tokenizer.save_pretrained("/mnt/e/medical-finetuned-done")

# loading the evaluations for text generator fine-tuned model
import evaluate
from tqdm import tqdm

rouge = evaluate.load("rouge")
bleu = evaluate.load("sacrebleu")

# gaining the last 2000 examples which haven't been seen during the training
dataset_for_test = load_dataset("FreedomIntelligence/medical-o1-verifiable-problem", split="train[-2000:]")

# loading the fine-tuned tokenizer and model
new_model_id = "/mnt/e/medical-finetuned-done"
new_tokenizer = AutoTokenizer.from_pretrained(new_model_id)
new_model = AutoModelForCausalLM.from_pretrained(new_model_id)

# formatting the test dataset for evaluation
dataset_for_test = dataset_for_test.map(formatting)
dataset_for_test = dataset_for_test.remove_columns([col for col in dataset_for_test.column_names if col != "text"])

new_tokenized_dataset = dataset_for_test.map(
    lambda sample: new_tokenizer(sample["text"], return_attention_mask=True),
    batched=True,
    remove_columns=["text"],
)

remainder = {"input_ids": [], "attention_mask": []}

new_test_dataset = new_tokenized_dataset.map(
    partial(chunk, chunk_length=512),
    batched=True,
)

# using the prepared test dataset to put it into GPU to speed up the evaluation, avoiding the gradient calculation with no_grad(),
# at the end computing the BLEU and ROUGE metric scores with comparing true answers and predicted answers by the fine-tuned model
predictions = []
references = []

for example in tqdm(new_test_dataset):
    input_ids = torch.tensor(example["input_ids"]).unsqueeze(0).to(new_model.device)
    attention_mask = torch.tensor(example["attention_mask"]).unsqueeze(0).to(new_model.device)
    with torch.no_grad():
        generated_ids = new_model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_new_tokens=100,
            eos_token_id=new_tokenizer.eos_token_id,
            pad_token_id=new_tokenizer.pad_token_id
        )
    pred = new_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    ref = new_tokenizer.decode(example["labels"], skip_special_tokens=True)
    predictions.append(pred)
    references.append([ref])

bleu_result = bleu.compute(predictions=predictions, references=references)
rouge_result = rouge.compute(predictions=predictions, references=[r[0] for r in references])

print(f"BLEU: {bleu_result['score']}")
print(f"ROUGE-1: {rouge_result['rouge1']}")
print(f"ROUGE-2: {rouge_result['rouge2']}")
print(f"ROUGE-L: {rouge_result['rougeL']}")

